{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "import collections\n",
    "import pickle\n",
    "import numpy as np\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "from gensim.test.utils import get_tmpfile\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import jieba\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainpath = \"D:\\\\Soong\\\\Soong\\\\textSummarization\\\\dataSet\\\\LCSTS\\\\CSV\\\\PART_II.csv\"\n",
    "validpath = \"D:\\\\Soong\\\\Soong\\\\textSummarization\\\\dataSet\\\\LCSTS\\\\CSV\\\\PART_III.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_list(path,clas):\n",
    "    data=pd.read_csv(path)\n",
    "    if clas=='article':\n",
    "        index=data.columns.values[0]\n",
    "    elif clas=='title':\n",
    "        index=data.columns.values[1]\n",
    "    listData=[]\n",
    "    for text in data[index]:\n",
    "\n",
    "        listData.append(text)\n",
    "        \n",
    "    return listData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setMax(path,clas):\n",
    "    data=pd.read_csv(path)\n",
    "    if clas=='article':\n",
    "        index=data.columns.values[0]\n",
    "    elif clas=='title':\n",
    "        index=data.columns.values[1]\n",
    "    num_tokens = [ len(tokens) for tokens in data[index] ]\n",
    "    num_tokens = np.array(num_tokens)\n",
    "    \n",
    "    # 取tokens平均值并加上两个tokens的标准差，\n",
    "    # 假设tokens长度的分布为正态分布，则max_tokens这个值可以涵盖95%左右的样本\n",
    "    max_tokens = np.mean(num_tokens) + 2 * np.std(num_tokens)\n",
    "    max_tokens = int(max_tokens)\n",
    "    return max_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dict(step):\n",
    "    if step==\"train\":\n",
    "        train_article=get_list(trainpath,'article')\n",
    "        train_title=get_list(trainpath,'title')\n",
    "        \n",
    "        text=train_article+train_title\n",
    "        words = list()\n",
    "        for per in text:\n",
    "\n",
    "\n",
    "            per=''.join(per)\n",
    "        #     print(per)\n",
    "            cut=jieba.lcut(per)\n",
    "        #     print(cut)\n",
    "            for precut in cut:\n",
    "                precut = re.sub(\"”：,'[\\s+\\.\\!\\/_,$%^*(+\\\"\\']+|[+——！，。？、~@#￥%……&*（）]+\", \"\",str(precut))\n",
    "\n",
    "                precut=precut.replace('”','').replace('：','').replace(',','').replace(',','').replace('[','').replace('[','')\n",
    "\n",
    "\n",
    "                precut = re.sub(u\"([^\\u4e00-\\u9fa5\\u0030-\\u0039\\u0041-\\u005a\\u0061-\\u007a])\",\"\",precut)\n",
    "\n",
    "        #     # 结巴分词\n",
    "        #     cut=jieba.lcut(per)\n",
    "\n",
    "        #     cut=''.join(cut)\n",
    "        #         print(cut)\n",
    "        #         print(precut)\n",
    "                words.append(precut)\n",
    "        word_counter=[]\n",
    "        for word in words:\n",
    "            if word not in word_counter:\n",
    "                word_counter.append(word)\n",
    "        # word_counter = set(words)\n",
    "\n",
    "        word_dict = dict()\n",
    "        word_dict[\"<padding>\"] = 0\n",
    "        word_dict[\"<unk>\"] = 1\n",
    "        word_dict[\"<s>\"] = 2\n",
    "        word_dict[\"</s>\"] = 3\n",
    "        for  word in word_counter:\n",
    "\n",
    "            word_dict[word] = len(word_dict)\n",
    "        with open(\"D:\\\\Soong\\\\Soong\\\\textSummarization\\\\ChineseNewsSum\\\\zijide\\\\word_dict.pickle\", \"wb\") as f:\n",
    "            pickle.dump(word_dict, f)\n",
    "    elif step == \"valid\":\n",
    "        with open(\"D:\\\\Soong\\\\Soong\\\\textSummarization\\\\ChineseNewsSum\\\\zijide\\\\word_dict.pickle\", \"rb\") as f:\n",
    "            word_dict = pickle.load(f)\n",
    "    reversed_dict = dict(zip(word_dict.values(), word_dict.keys()))\n",
    "    \n",
    "\n",
    "    \n",
    "    article_max_len = setMax(trainpath,'article')\n",
    "    summary_max_len = setMax(trainpath,'title')\n",
    "\n",
    "    return word_dict, reversed_dict, article_max_len, summary_max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train.py\n",
    "word_dict, reversed_dict, article_max_len, summary_max_len = build_dict(\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(step, word_dict, article_max_len, summary_max_len):\n",
    "    if step==\"train\":\n",
    "        article=get_list(trainpath,'article')\n",
    "        title=get_list(trainpath,'title')\n",
    "        \n",
    "        \n",
    "    elif step == \"valid\":\n",
    "        article = get_list(validpath,'article')\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    x = [jieba.lcut(''.join(d)) for d in article]\n",
    "            \n",
    "    x = [[word_dict.get(w, word_dict[\"<unk>\"]) for w in d] for d in x]\n",
    "    x = [d[:article_max_len] for d in x]\n",
    "    x = [d + (article_max_len - len(d)) * [word_dict[\"<padding>\"]] for d in x]\n",
    "    \n",
    "    if step == \"valid\":\n",
    "        return x\n",
    "    else:        \n",
    "        y = [jieba.lcut(''.join(d)) for d in title]\n",
    "        y = [[word_dict.get(w, word_dict[\"<unk>\"]) for w in d] for d in y]\n",
    "        y = [d[:(summary_max_len - 1)] for d in y]\n",
    "        return x, y\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train.py\n",
    "train_x, train_y = build_dataset(\"train\", word_dict, article_max_len, summary_max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
