from selenium import webdriver
import requests
import random
from bs4 import BeautifulSoup
import urllib.request
import time
from selenium.webdriver.common.desired_capabilities import DesiredCapabilities
from lxml.html import fromstring
####define new userAgent

def userAgent():
    user_agents = [
    "Mozilla/5.0 (Linux; U; Android 2.3.6; en-us; Nexus S Build/GRK39F) AppleWebKit/533.1 (KHTML, like Gecko) Version/4.0 Mobile Safari/533.1",
    "Avant Browser/1.2.789rel1 (http://www.avantbrowser.com)",
    "Mozilla/5.0 (Windows; U; Windows NT 6.1; en-US) AppleWebKit/532.5 (KHTML, like Gecko) Chrome/4.0.249.0 Safari/532.5",
    "Mozilla/5.0 (Windows; U; Windows NT 5.2; en-US) AppleWebKit/532.9 (KHTML, like Gecko) Chrome/5.0.310.0 Safari/532.9",
    "Mozilla/5.0 (Windows; U; Windows NT 5.1; en-US) AppleWebKit/534.7 (KHTML, like Gecko) Chrome/7.0.514.0 Safari/534.7",
    "Mozilla/5.0 (Windows; U; Windows NT 6.0; en-US) AppleWebKit/534.14 (KHTML, like Gecko) Chrome/9.0.601.0 Safari/534.14",
    "Mozilla/5.0 (Windows; U; Windows NT 6.1; en-US) AppleWebKit/534.14 (KHTML, like Gecko) Chrome/10.0.601.0 Safari/534.14",
    "Mozilla/5.0 (Windows; U; Windows NT 6.1; en-US) AppleWebKit/534.20 (KHTML, like Gecko) Chrome/11.0.672.2 Safari/534.20",
    "Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/534.27 (KHTML, like Gecko) Chrome/12.0.712.0 Safari/534.27",
    "Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/535.1 (KHTML, like Gecko) Chrome/13.0.782.24 Safari/535.1",
    "Mozilla/5.0 (Windows NT 6.0) AppleWebKit/535.2 (KHTML, like Gecko) Chrome/15.0.874.120 Safari/535.2",
    "Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/535.7 (KHTML, like Gecko) Chrome/16.0.912.36 Safari/535.7",
    "Mozilla/5.0 (Windows; U; Windows NT 6.0 x64; en-US; rv:1.9pre) Gecko/2008072421 Minefield/3.0.2pre",
    "Mozilla/5.0 (Windows; U; Windows NT 5.1; en-US; rv:1.9.0.10) Gecko/2009042316 Firefox/3.0.10",
    "Mozilla/5.0 (Windows; U; Windows NT 6.0; en-GB; rv:1.9.0.11) Gecko/2009060215 Firefox/3.0.11 (.NET CLR 3.5.30729)",
    "Mozilla/5.0 (Windows; U; Windows NT 6.0; en-US; rv:1.9.1.6) Gecko/20091201 Firefox/3.5.6 GTB5",
    "Mozilla/5.0 (Windows; U; Windows NT 5.1; tr; rv:1.9.2.8) Gecko/20100722 Firefox/3.6.8 ( .NET CLR 3.5.30729; .NET4.0E)",
    "Mozilla/5.0 (Windows NT 6.1; rv:2.0.1) Gecko/20100101 Firefox/4.0.1",
    "Mozilla/5.0 (Windows NT 6.1; Win64; x64; rv:2.0.1) Gecko/20100101 Firefox/4.0.1",
    "Mozilla/5.0 (Windows NT 5.1; rv:5.0) Gecko/20100101 Firefox/5.0",
    "Mozilla/5.0 (Windows NT 6.1; WOW64; rv:6.0a2) Gecko/20110622 Firefox/6.0a2",
    "Mozilla/5.0 (Windows NT 6.1; WOW64; rv:7.0.1) Gecko/20100101 Firefox/7.0.1",
    "Mozilla/5.0 (Windows NT 6.1; WOW64; rv:2.0b4pre) Gecko/20100815 Minefield/4.0b4pre",
    "Mozilla/4.0 (compatible; MSIE 5.5; Windows NT 5.0 )", ]

    UserAgent = random.choice(user_agents)
    return UserAgent
def getIp():

    ip = [
    "http://202.159.118.50:80",
    "http://41.210.252.16:8080",
    "http://159.148.119.39:3128",
    "http://204.16.1.182:3128",
    "http://129.107.60.14:80",
    "http://200.65.129.2:80",
    "http://67.69.254.240:80",
    "http://193.37.152.206:3128",
    "http://169.235.24.133:3127",
    "http://200.204.154.29:6588",
    "http://200.174.85.195:3128",
    "http://67.69.254.248:80",
    "http://67.69.254.243:80",
    "http://152.101.118.253:8080",
    "http://189.112.246.145:8080",
    "http://79.148.249.246:8080",
    ]

    IP = random.choice(ip)
    return IP
def getHTMLText(url1):
    try:
        agent='Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/69.0.3497.100 Safari/537.36'
        cookie='CMSPreferredCulture=en-US; promo_code=zhs3nmqq; AMCV_05B1470957C7F5EB7F000101%40AdobeOrg=T; _gcl_au=1.1.1146345835.1537856785; _ga=GA1.2.79711517.1537856785; _gid=GA1.2.312422580.1537856785; CookieConsent=-1; sc_prp20=Less%20than%201%20day; ASP.NET_SessionId=ek1lqw2swphydgpcxyazslrv; BIGipServernewsmax_2_farm_443_pool=672114880.47873.0000; AMCVS_05B1470957C7F5EB7F000101%40AdobeOrg=1; _sdsat_content: server=www.newsmax.com; sc_evr61=Repeat; s_cc=true; CMSCurrentTheme=Newsmax; NMSeg=FR,PR; sc_daysSinceLastVisit_s=Less%20than%201%20day; loadtime=nmx:home:index|7.64; AMCV_05B1470957C7F5EB7F000101%40AdobeOrg=102365995%7CMCIDTS%7C17801%7CMCMID%7C00079303136662577902122389947097983065%7CMCAAMLH-1538537968%7C11%7CMCAAMB-1538537968%7CRKhpRz8krg2tLO6pguXWp5olkAcUniQYPHaMWWgdJ3xzPWQmdj0y%7CMCOPTOUT-1537940368s%7CNONE%7CMCAID%7CNONE%7CvVersion%7C2.2.0; _dc_gtm_UA-31221-1=1; sc_previousPageName=nmx%3Aus%3Aarticle%3Asecond_kavanaugh_accuser_balks_at_senate_grilling%3A883432; s_ppvl=nmx%253Ahome%253Aindex%2C15%2C9%2C1090%2C1536%2C190%2C1536%2C864%2C1.25%2CP; s_ppv=nmx%253Aus%253Aarticle%253Asecond_kavanaugh_accuser_balks_at_senate_grilling%253A883432%2C4%2C4%2C190%2C1536%2C190%2C1536%2C864%2C1.25%2CP; sc_getNewRepeat=1537933170033-Repeat; sc_daysSinceLastVisit=1537933170034; s_sq=%5B%5BB%5D%5D'
        refer='https://www.newsmax.com/us/brett-kavanaugh-sexual-misconduct-allegations-senate/2018/09/25/id/883432/'
        headers={'User-Agent':agent,'Cookie':cookie,'Refer':refer}

        r = requests.get(url1,timeout = 30)
        r.raise_for_status()
        r.encoding = 'utf-8'
        return r.text
    except:
        return ""




def getAllUrl():
    ###give url and open
    option = webdriver.ChromeOptions()
    # option.add_argument('user-agent=userAgent()')
    # option.add_argument('--proxy-server='+str(getIp()))
    browser = webdriver.Chrome()
    browser.get("https://finance.yahoo.com/")
    count = 0
    while(count<=10000):
    # while (count <= 10):
        option.add_argument('user-agent=userAgent()')
        option.add_argument('--proxy-server=' + str(getIp()))
        browser.execute_script("window.scrollBy(0,300)")
        time.sleep(10)
        count =count+1
        print (time.strftime("%Y-%m-%d %H:%M:%S", time.localtime()))
        print(count)
    for link in browser.find_elements_by_xpath("//h3[@class='Mb(5px)']/a[@href]"):

            if (link.get_attribute('href').find('/m/') > 0 or link.get_attribute('href').find('/video/') > 0):
             a = 0
            else:
                AllUrl = link.get_attribute('href')
                html = getHTMLText(AllUrl)
                # print(html)
                soup = BeautifulSoup(html, "html.parser")
                title = soup.select("header > h1")
                title = str(title).replace(
                    '[<h1 class="Lh(36px) Fz(25px)--sm Fz(32px) Mb(17px)--sm Mb(20px) Mb(30px)--lg Ff($ff-primary) Lts($lspacing-md) Fw($fweight) Fsm($fsmoothing) Fsmw($fsmoothing) Fsmm($fsmoothing) Wow(bw)" data-reactid="3" itemprop="headline">',
                    '')
                title = str(title).replace('</h1>]', '')
                # print(title)
                txtName = title
                f = open('D:/study/master/pydata/' + txtName + '.txt', 'a+', encoding='utf-8')
                f.write('title:' + str(title))
                f.write(str('\n'))
                # print(title[0].get_text())

                ######date
                if (soup.select("div > time")):
                    date = soup.select("div > time")[0]
                    date = str(date).replace('<time class="date Fz(11px) Mb(4px) D(ib)" data-reactid="15"', '')
                    date = str(date).replace(' itemprop="datePublished">', '')
                    date = str(date).replace('</time>', '')
                    f.write('Date:' + str(date))
                    f.write(str('\n'))
                    # print(date)

                #######authou
                author = soup.select("div > div.author-name")
                author = str(author).replace(
                    '[<div class="author-name Lh(18px) Td(n) Fw(b) Fz(12px) C(#000) Mend(3px)" data-reactid="9" itemprop="name">',
                    '')
                author = str(author).replace('</div>]', '')
                f.write('author:' + str(author))
                f.write(str('\n'))
                # print (author)

                press = soup.select("span > a")
                press = str(press).replace('[<a class="Fz(12px) C(#222) Fw(b)" data-reactid="13"', '')
                press = str(press).replace(
                    '" itemprop="url" rel="noopener noreferrer nofollow" target="_blank">InvestorPlace</a>, <a class="Va(m) Fw(b) Fz(s) C(black) Wow(bw) Us(n)" data-reactid="4" data-test="list-name" href="',
                    '')
                press = str(press).replace('" title="Recently Viewed"><span data-reactid="5">', '')
                press = str(press).replace(
                    '-.28 1.834-.795l12.5-13.145L20.95 9.775z" data-reactid="7"></path></svg></a>]', '')
                press = str(press).replace(
                    '" viewbox="0 0 48 48" width="14"><path d="M20.95 9.774c-.966-1.025-2.694-1.023-3.67 0-.98 1.032-.98 2.71 0 3.74l8.95 9.412-8.946 9.404c-.983 1.036-.98 2.716 0 3.74.49.514 1.14.796 1.837.796.698 0 1.35',
                    '')
                press = str(press).replace(
                    '</span><svg class="Px(2px) Pt(3px) Cur(p)" data-icon="caret-right-finance" data-reactid="6" height="14" style="fill:#000;stroke:#000;stroke-width:0;vertical-align:bottom;"',
                    '')
                f.write('Press:' + str(press))
                # print(press)
                f.write(str('\n'))

                content = soup.select("div > p")
                for con in content:

                    if len(con) > 0:
                        # print(con.get_text())
                        con1 = con.get_text()
                        f.write(str(con1))
                f.write(str('\n'))
                f.close()
                # AllUrl = link.get_attribute('href')
                # html = getHTMLText(AllUrl)
                # # print(html)
                # soup = BeautifulSoup(html, "html.parser")
                # title = soup.select("header.canvas-header > h1")
                # # print(title)
                # print(title[0].get_text())
                #
                # content= soup.select("article > p")
                # for con in content:
                #     if len(con) > 0:
                #         print(con.get_text())
                time.sleep(6)








   

#### forward
# driver.forward()
if __name__=="__main__":
    getAllUrl()




